{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87acacf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c069f59",
   "metadata": {},
   "source": [
    "## Create the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bccc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install -U datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0770733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4d11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bda0f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fba04246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    ids = enc.encode_ordinary(example[\"text\"])\n",
    "    out = {'ids':ids, 'len':len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=[\"text\"],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "    )\n",
    "    # concatenate all the ids in each dataset into one large\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'],dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype=np.uint16\n",
    "        arr =  np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True)\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            arr[idx:idx+len(arr_batch)] = arr_batch\n",
    "            idx+= len(arr_batch)\n",
    "        arr.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3f805cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        x = x.pin_memory().to(device, non_blocking=True)\n",
    "        y = y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e46fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a9d521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99a26145",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    block_size=128,\n",
    "    vocab_size=50257,\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15bf3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce192a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2650dcb2370>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_iters = 20000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76144b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shres\\AppData\\Local\\Temp\\ipykernel_20192\\3141452839.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "model = GPT(config)\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c03cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8fdee127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2650dcb2370>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_iters = 20000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-5\n",
    "eval_iters = 200\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f37a4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "## put in weight decay, changed beta2 to 0.95\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer=optimizer, total_iters=warmup_steps) #Implement linear warmup\n",
    "schedulerr_decay = CosineAnnealingLR(optimizer=optimizer, T_max=max_iters - warmup_steps, eta_min=min_lr) #Implement lr decay\n",
    "\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, schedulerr_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52963f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0835e90f960f467eb2770a7446f0c572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: train loss 9.7505, val loss 9.7529\n",
      "The current learning rate: 0.00005\n",
      "Epoch 400: train loss 9.4320, val loss 9.4375\n",
      "The current learning rate: 0.00006\n",
      "Epoch 600: train loss 9.1205, val loss 9.1286\n",
      "The current learning rate: 0.00007\n",
      "Epoch 800: train loss 8.6990, val loss 8.7048\n",
      "The current learning rate: 0.00009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\miniconda3\\envs\\medsam\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 8.3211, val loss 8.3234\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1200: train loss 7.9471, val loss 7.9475\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1400: train loss 7.5941, val loss 7.5877\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1600: train loss 7.2042, val loss 7.2031\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1800: train loss 6.8928, val loss 6.8927\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2000: train loss 6.6118, val loss 6.6068\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2200: train loss 6.3346, val loss 6.3399\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2400: train loss 6.0714, val loss 6.0652\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2600: train loss 5.8512, val loss 5.8480\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2800: train loss 5.6618, val loss 5.6663\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3000: train loss 5.4892, val loss 5.4911\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3200: train loss 5.3100, val loss 5.3197\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3400: train loss 5.1760, val loss 5.1815\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3600: train loss 5.0517, val loss 5.0397\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3800: train loss 4.9339, val loss 4.9396\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4000: train loss 4.8264, val loss 4.8313\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4200: train loss 4.7283, val loss 4.7364\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4400: train loss 4.6478, val loss 4.6556\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4600: train loss 4.5757, val loss 4.5789\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4800: train loss 4.4931, val loss 4.4917\n",
      "The current learning rate: 0.00010\n",
      "Epoch 5000: train loss 4.4233, val loss 4.4159\n",
      "The current learning rate: 0.00009\n",
      "Epoch 5200: train loss 4.3617, val loss 4.3536\n",
      "The current learning rate: 0.00009\n",
      "Epoch 5400: train loss 4.3070, val loss 4.2955\n",
      "The current learning rate: 0.00009\n",
      "Epoch 5600: train loss 4.2399, val loss 4.2331\n",
      "The current learning rate: 0.00009\n",
      "Epoch 5800: train loss 4.1862, val loss 4.1854\n",
      "The current learning rate: 0.00009\n",
      "Epoch 6000: train loss 4.1518, val loss 4.1464\n",
      "The current learning rate: 0.00009\n",
      "Epoch 6200: train loss 4.0847, val loss 4.0943\n",
      "The current learning rate: 0.00009\n",
      "Epoch 6400: train loss 4.0523, val loss 4.0364\n",
      "The current learning rate: 0.00009\n",
      "Epoch 6600: train loss 3.9988, val loss 4.0059\n",
      "The current learning rate: 0.00009\n",
      "Epoch 6800: train loss 3.9660, val loss 3.9694\n",
      "The current learning rate: 0.00009\n",
      "Epoch 7000: train loss 3.9326, val loss 3.9286\n",
      "The current learning rate: 0.00009\n",
      "Epoch 7200: train loss 3.8819, val loss 3.8865\n",
      "The current learning rate: 0.00009\n",
      "Epoch 7400: train loss 3.8570, val loss 3.8438\n",
      "The current learning rate: 0.00009\n",
      "Epoch 7600: train loss 3.8291, val loss 3.8308\n",
      "The current learning rate: 0.00009\n",
      "Epoch 7800: train loss 3.7847, val loss 3.7973\n",
      "The current learning rate: 0.00009\n",
      "Epoch 8000: train loss 3.7636, val loss 3.7626\n",
      "The current learning rate: 0.00009\n",
      "Epoch 8200: train loss 3.7329, val loss 3.7368\n",
      "The current learning rate: 0.00008\n",
      "Epoch 8400: train loss 3.7024, val loss 3.7213\n",
      "The current learning rate: 0.00008\n",
      "Epoch 8600: train loss 3.6878, val loss 3.6819\n",
      "The current learning rate: 0.00008\n",
      "Epoch 8800: train loss 3.6493, val loss 3.6596\n",
      "The current learning rate: 0.00008\n",
      "Epoch 9000: train loss 3.6322, val loss 3.6373\n",
      "The current learning rate: 0.00008\n",
      "Epoch 9200: train loss 3.6148, val loss 3.6040\n",
      "The current learning rate: 0.00008\n",
      "Epoch 9400: train loss 3.5870, val loss 3.5902\n",
      "The current learning rate: 0.00008\n",
      "Epoch 9600: train loss 3.5609, val loss 3.5783\n",
      "The current learning rate: 0.00008\n",
      "Epoch 9800: train loss 3.5319, val loss 3.5506\n",
      "The current learning rate: 0.00008\n",
      "Epoch 10000: train loss 3.5209, val loss 3.5231\n",
      "The current learning rate: 0.00008\n",
      "Epoch 10200: train loss 3.5059, val loss 3.5122\n",
      "The current learning rate: 0.00008\n",
      "Epoch 10400: train loss 3.4899, val loss 3.4932\n",
      "The current learning rate: 0.00008\n",
      "Epoch 10600: train loss 3.4655, val loss 3.4755\n",
      "The current learning rate: 0.00007\n",
      "Epoch 10800: train loss 3.4478, val loss 3.4626\n",
      "The current learning rate: 0.00007\n",
      "Epoch 11000: train loss 3.4338, val loss 3.4355\n",
      "The current learning rate: 0.00007\n",
      "Epoch 11200: train loss 3.4174, val loss 3.4182\n",
      "The current learning rate: 0.00007\n",
      "Epoch 11400: train loss 3.4116, val loss 3.4010\n",
      "The current learning rate: 0.00007\n",
      "Epoch 11600: train loss 3.3910, val loss 3.3915\n",
      "The current learning rate: 0.00007\n",
      "Epoch 11800: train loss 3.3768, val loss 3.3703\n",
      "The current learning rate: 0.00007\n",
      "Epoch 12000: train loss 3.3610, val loss 3.3642\n",
      "The current learning rate: 0.00007\n",
      "Epoch 12200: train loss 3.3364, val loss 3.3495\n",
      "The current learning rate: 0.00007\n",
      "Epoch 12400: train loss 3.3477, val loss 3.3383\n",
      "The current learning rate: 0.00007\n",
      "Epoch 12600: train loss 3.3175, val loss 3.3389\n",
      "The current learning rate: 0.00007\n",
      "Epoch 12800: train loss 3.3107, val loss 3.3123\n",
      "The current learning rate: 0.00007\n",
      "Epoch 13000: train loss 3.2902, val loss 3.3026\n",
      "The current learning rate: 0.00006\n",
      "Epoch 13200: train loss 3.2781, val loss 3.2848\n",
      "The current learning rate: 0.00006\n",
      "Epoch 13400: train loss 3.2798, val loss 3.2660\n",
      "The current learning rate: 0.00006\n",
      "Epoch 13600: train loss 3.2645, val loss 3.2697\n",
      "The current learning rate: 0.00006\n",
      "Epoch 13800: train loss 3.2410, val loss 3.2546\n",
      "The current learning rate: 0.00006\n",
      "Epoch 14000: train loss 3.2384, val loss 3.2448\n",
      "The current learning rate: 0.00006\n",
      "Epoch 14200: train loss 3.2281, val loss 3.2447\n",
      "The current learning rate: 0.00006\n",
      "Epoch 14400: train loss 3.2101, val loss 3.2297\n",
      "The current learning rate: 0.00006\n",
      "Epoch 14600: train loss 3.2135, val loss 3.2147\n",
      "The current learning rate: 0.00006\n",
      "Epoch 14800: train loss 3.1973, val loss 3.2106\n",
      "The current learning rate: 0.00006\n",
      "Epoch 15000: train loss 3.1900, val loss 3.1939\n",
      "The current learning rate: 0.00006\n",
      "Epoch 15200: train loss 3.1653, val loss 3.1863\n",
      "The current learning rate: 0.00006\n",
      "Epoch 15400: train loss 3.1671, val loss 3.1780\n",
      "The current learning rate: 0.00006\n",
      "Epoch 15600: train loss 3.1583, val loss 3.1499\n",
      "The current learning rate: 0.00006\n",
      "Epoch 15800: train loss 3.1620, val loss 3.1575\n",
      "The current learning rate: 0.00006\n",
      "Epoch 16000: train loss 3.1359, val loss 3.1432\n",
      "The current learning rate: 0.00006\n",
      "Epoch 16200: train loss 3.1411, val loss 3.1450\n",
      "The current learning rate: 0.00005\n",
      "Epoch 16400: train loss 3.1285, val loss 3.1356\n",
      "The current learning rate: 0.00005\n",
      "Epoch 16600: train loss 3.1298, val loss 3.1301\n",
      "The current learning rate: 0.00005\n",
      "Epoch 16800: train loss 3.1278, val loss 3.1143\n",
      "The current learning rate: 0.00005\n",
      "Epoch 17000: train loss 3.1039, val loss 3.1160\n",
      "The current learning rate: 0.00005\n",
      "Epoch 17200: train loss 3.1119, val loss 3.1101\n",
      "The current learning rate: 0.00005\n",
      "Epoch 17400: train loss 3.0878, val loss 3.0959\n",
      "The current learning rate: 0.00005\n",
      "Epoch 17600: train loss 3.0824, val loss 3.0836\n",
      "The current learning rate: 0.00005\n",
      "Epoch 17800: train loss 3.0737, val loss 3.0781\n",
      "The current learning rate: 0.00005\n",
      "Epoch 18000: train loss 3.0818, val loss 3.0600\n",
      "The current learning rate: 0.00005\n",
      "Epoch 18200: train loss 3.0617, val loss 3.0663\n",
      "The current learning rate: 0.00005\n",
      "Epoch 18400: train loss 3.0589, val loss 3.0570\n",
      "The current learning rate: 0.00005\n",
      "Epoch 18600: train loss 3.0523, val loss 3.0519\n",
      "The current learning rate: 0.00005\n",
      "Epoch 18800: train loss 3.0368, val loss 3.0487\n",
      "The current learning rate: 0.00005\n",
      "Epoch 19000: train loss 3.0353, val loss 3.0384\n",
      "The current learning rate: 0.00005\n",
      "Epoch 19200: train loss 3.0349, val loss 3.0367\n",
      "The current learning rate: 0.00005\n",
      "Epoch 19400: train loss 3.0210, val loss 3.0299\n",
      "The current learning rate: 0.00005\n",
      "Epoch 19600: train loss 3.0173, val loss 3.0144\n",
      "The current learning rate: 0.00005\n",
      "Epoch 19800: train loss 3.0121, val loss 3.0078\n",
      "The current learning rate: 0.00005\n"
     ]
    }
   ],
   "source": [
    "## Pre-train the SLM\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch%eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val']<best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "        \n",
    "    X, y = get_batch('train')\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X,y)\n",
    "        loss = loss/gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    if((epoch+1)%gradient_accumulation_steps ==0) or (epoch+1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ad98bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list_converted = [i.cpu().detach().numpy() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "\n",
    "plt.xlabel('Steps - Every 100 epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4239ebb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run SLM inference on our trained model\n",
    "\n",
    "model = GPT(config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ca049ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a8010da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin that started to open. It looked embarrassed because it could not find something yummy. It didn't come inside against the window anymore.\n",
      "\n",
      "Theray was so colorful, but the wand was both missed who always look for herSuddenly, she heard a gift. She started to smile and she stacked itdrop. She saw that the swam was a big, pose.\n",
      "\n",
      "Mia splashed down one mistake, especially brave ' Contextuse me see outside, you can help you.\" They went to the park asked the bar.\n",
      "\n",
      "So, they reached the doctor picked up on the kitchen, andily was so troubled. She was happy and strong, gracefully!\n",
      "\n",
      "But west was so free than Lily masseirl and hot gate nodded. She smiled and decided to put on her turn under her mug and skip together in the ocean. The end.Once upon a time, there was an Whenever it was towards- skating and hide. The ancient were forgotten near the sun and\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim=0))\n",
    "\n",
    "y = model.generate(context, 200)\n",
    "\n",
    "print(enc.decode(y[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
